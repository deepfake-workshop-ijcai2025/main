<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Workshop Challenge | IJCAI 2025</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
<nav>
    <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="schedule.html">Schedule</a></li>
        <li class="dropdown">
            <a href="#">Challenge</a>
            <ul class="dropdown-menu">
                <li><a href="challenge.html">Evaluate</a></li>
                <li><a href="datasets.html">Datasets</a></li>
            </ul>
        </li>
        <li><a href="Registration.html">Registration</a></li>
        <li><a href="organizers.html">Organizers</a></li>
    </ul>
</nav>

<div class="container">
    <div class="page-content">
        <div class="card committee-section">
            <h2 class="section-header"><b>Challenge Significance</b></h2>
            <p>As deepfake technology advances rapidly, fake images and audio-visual content threaten social security and media credibility. The Deepfake Detection and Localization Challenge (DDL Challenge) aims to:</p>
            <ul>
                <li>Enhance detection interpretability by providing intuitive evidence through temporal-spatial localization (e.g., pixel-level tampered areas, forged timestamps).</li>
                <li>Address multi-modal risks by tackling complex attacks such as "forged audio + authentic video," filling the existing technical gaps.</li>
                <li>Promote technology inclusivity by providing access to the world's largest multi-modal deepfake dataset (1.8M+ samples), which encompasses 88 forgery techniques.</li>
            </ul>
        </div>

        <div class="card committee-section">
            <h2 class="section-header"><b>Competition Rules and Incentives</b></h2>
            <h3>1. Participation Guidelines</h3>
            <h4>a) Model Submission Requirements</h4>
            <ul>
                <li>Each track permits only one model submission that must simultaneously address both classification and localization tasks.</li>
                <li>All models must utilize open-source pre-trained architectures. Teams developing proprietary models during the competition are required to publicly release their model specifications and training protocols under open-source licenses (e.g., MIT, Apache 2.0) during the competition period.</li>
                <li>Winning solutions must open-source their full implementation, including:
                    <ul>
                        <li>Training pipelines and hyperparameter configurations</li>
                        <li>Evaluation code with reproducibility documentation</li>
                        <li>Final model weights in standard formats</li>
                    </ul>
                </li>
                <li>Violations of these rules will result in disqualification. The organizing committee reserves final authority over all competition-related matters.</li>
                <li>Extended samples generated by data augmentation/deepfake tools based on the released training set can be used for training, but these tools need to be submitted for reproduction.</li>
            </ul>
            <h3>2. Awards and Recognition</h3>
            <ul>
                <li>Monetary prizes: Substantial monetary awards will be granted to top-performing teams across both tracks.</li>
                <li>Academic recognition: Exceptional solutions will be invited for presentation at the IJCAI.</li>
            </ul>
        </div>

        <div class="card committee-section">
            <h2 class="section-header"><b>Challenge Content</b></h2>
            <p>This challenge consists of two tracks focusing on detection and localization of deepfake artifacts:</p>
            <h3>Track 1: Image Detection and Localization (DDL-I)</h3>
            <ul>
                <li>Tasks: Real/Fake Classification (Cla) + Spatial Localization (SL).</li>
                <li>Dataset: Over 1.5 million images covering 61 manipulation techniques, including single-face and multi-face tampering scenarios.</li>
                <li>Evaluation Metrics: Area Under the ROC Curve (AUC) for detection, F1 Score, and Intersection over Union (IoU) for spatial localization (calculated exclusively for fake samples).</li>
                <center><img src="images/f1_score.png" style="width: 500px; height: auto;"></center>
            </ul>

            <h3>Track 2: Audio-Visual Detection and Localization (DDL-AV)</h3>
            <ul>
                <li>Tasks: Real/Fake Classification (Cla) + Temporal Localization (TL).</li>
                <li>Dataset: 300,000+ samples integrating 9 audio manipulation methods and 18 video forgery techniques.</li>
                <li>Evaluation Metrics: Area Under the ROC Curve (AUC) for detection, Average Precision (AP), and Average Recall (AR) for temporal localization (calculated exclusively for fake samples).</li>
                <center><img src="images/average_percision.png" style="width: 500px; height: auto;"></center>
            </ul>
        </div>
    </div>
<div class="card committee-section">
    <h2 class="section-header"><b>Submission Formats</b></h2>

    <h3>Image Track Submission Format</h3>
    <ul>
        <li><b>Real:</b> The image is real and the corresponding label is 0.</li>
        <li><b>Fake:</b> The image is fake and the corresponding label is 1.</li>
        <li><b>Detection:</b> The metrics for this task is the AUC score.</li>
        <li><b>Localization:</b> The metrics for this task is the IoU score and F1 score.</li>
        <li>The Final score is the average of the above three scores.</li>
        <center><img src="images/final_score1.jpg" style="width: 500px; height: auto;"></center>
        <li>The model output should be a single confidence number of the input image being fake and the corresponding predicted manipulated mask. The expected submission format is like below:</li>
    </ul>
    <center><img src="images/image_format.png" style="width: 500px; height: auto;"></center>
    <p>The submitted forlder format is like below:
        -prediction.txt
        -mask (folder)
        -000001.png
        -000002.png
        -000003.png
        ...
        -xxxxxx.png</p>
    <p>Note: The size of the predicted mask should be the same as the corresponding original image.</p>
    <span style="color: red;"><b>Then compress them into a .zip file.</b></span>

    <h3>Audio-Visual Track Submission Format</h3>
    <ul>
        <li><b>Real:</b> The video is real, which means there is no fake segment in the video.</li>
        <li><b>Fake:</b> The video is fake, which means there is at least one fake segment in the video.</li>
        <li><b>Detection:</b> The metrics for this task is the AUC score.</li>
        <li><b>Localization:</b> The metrics for this task are AP (Average Precision) and AR (Average Recall).</li>
        <center><img src="images/audio_format_1.png" style="width: 500px; height: auto;"></center>
        <P>The Final score is the average of the above two scores.</P>
        <center><img src="images/final_score2.jpg" style="width: 500px; height: auto;"></center>
        <Li>The model output should be a single confidence number of the input video being fake and the temporal localization of the fake segments in the input video. The expected format is a txt file and a json file with the following structure:</Li>
    </ul>
    <center><img src="images/audio_format_2.png" style="width: 500px; height: auto;"></center>
    <span style="color: red;"><b>Then compress them into a .zip file.</b></span>
</div>
</div>
</div>
<footer>
    <p>Contact: workshop-deepfake@ijcai2025.org</p>
</footer>
</body>
</html>