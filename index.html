<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deepfake Workshop 2025</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</head>

<body>
<nav>
    <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="schedule.html">Schedule</a></li>
        <li class="dropdown">
            <a href="#">Challenge</a>
            <ul class="dropdown-menu">
                <li><a href="challenge.html">Evaluate</a></li>
                <li><a href="datasets.html">Datasets</a></li>
            </ul>
        </li>
        <li><a href="Registration.html">Registration</a></li>
        <li><a href="organizers.html">Organizers</a></li>
    </ul>
</nav>

<div class="front-img" style="background-image: url('https://2025.ijcai.org/wp-content/uploads/2020/07/Ijcai2021-Header-Image.jpg');
                                   background-size: cover; background-position: center; height: 500px;">
    <div class="ast-container" style="text-align: center;">
        <img src="https://2025.ijcai.org/wp-content/themes/astra-child/img/IJCAI2025_Montreal_Logo.png" alt="IJCAI 2025">
    </div>
</div>

<header class="hero">
    <h1>Workshop on Deepfake Detection, Localization, and Interpretability</h1>
    <p>IJCAI 2025 | August 16, 2025 | Montreal/Guangzhou</p>
</header>

<main class="container">
    <!-- News Section -->
    <section class="highlights" style="margin-bottom: 2rem;">
        <div class="highlight-card">
        <h2 class="section-header" style="text-align: left; color:red;">
            <span class="icon">📄</span>
            News
        </h2>
            <div style="text-align: left;"> <!-- 内容左对齐 -->
                <p>🔥🔥 Our workshop programs for Montreal and Guangzhou are released. Check them out here[https://deepfake-workshop-ijcai2025.github.io/main/schedule.html]</p>
                <p>🔥🔥 Generative Large Model Security Challenge Launches — More Details Available on the <a href="https://tianchi.aliyun.com/competition/entrance/532362?spm=a2c22.29524702.0.0.5f7c526eYhnLW6" target="_blank">Tianchi Competition Platform</a> !!!</p>
                <p>🔥🔥 12 May: Test sets are released. Please check your registered email for more information. Submissions are open until 21 May.</p>
                <p> 16 Apr: The paper submission site is online.</p>
                <p> 5 Mar: The Deepfake Detection, Localization and Interpretability Workshop and Competition has been accepted by IJCAI 2025.</p>
                <a href="https://chairingtool.com/conferences/ijcai25-w04/main-track?role=author" target="_blank">
                        https://chairingtool.com/conferences/ijcai25-w04/main-track?role=author</a>. 
                    Please check the Call for Papers section for related research topics.
                </p>
            <p>More news will be added here shortly.</p>
        </div>
            </div>
    </section>

    <!-- Description Section -->
    <section class="highlight-card" style="margin-bottom: 2rem; text-align: left;">
        <h2 class="section-header" style="text-align: left;">
            <span class="icon">📄</span>
            Description
        </h2>

        <p>This workshop is held to address the new security challenges brought about by the rapid development of AI-generated content technology. As <strong>Deepfake technology</strong> expands from single face synthesis to multimodal content generation, the reliability of existing detection methods in real complex scenarios faces severe challenges.</p>

        <p>Although traditional black box classification models can output probability judgments, they cannot provide a visual chain of evidence that convinces law enforcement agencies. At present, <strong>explanatory research</strong> based on large language models is still in the exploratory stage, lacking a systematic theoretical framework and evaluation standards.</p>

        <p>By bringing together top scholars from around the world to jointly overcome core technical bottlenecks such as multimodal forgery positioning, weakly supervised explanation generation, and cross-scenario generalization detection, this workshop will promote the establishment of a verifiable and traceable Deepfake analysis system.</p>

        <p>Aligned with <strong>IJCAI’s mission</strong> of "AI for Good", this "Deepfake Detection, Localization, and Interpretability" workshop is meticulously designed to promote progress in the AI safety community, particularly emphasizing advancing the state-of-the-art in interpretable Deepfake detection tasks. The workshop will feature a combination of invited talks, paper presentations, and competition results.</p>

        <p>Sessions will include:</p>
        <ul class="member-details">
            <li>Keynote presentations from leading researchers</li>
            <li>Technical paper sessions with oral presentations</li>
            <li>Competition results and analysis</li>
        </ul>
    </section>

    <!-- Call for Papers Section with Key Topics and Related Research Topics -->
    <div class="highlight-card" style="padding: 2rem;">
        <h2 class="section-header" style="text-align: left;">
            <span class="icon">📄</span>
            Call for Papers
        </h2>

        <!-- 描述内容放在标题上方 -->
        <p style="text-align: left;">We welcome research papers of the following topics, as well as papers detailing the solutions used in our competition.
            Top solutions and best papers will be invited to our workshops for oral presentations. Please go to
            <a href="https://chairingtool.com/conferences/ijcai25-w04/main-track?role=author" target="_blank">
            https://chairingtool.com/conferences/ijcai25-w04/main-track?role=author</a> for paper submission before May 28. All the published paper are also welcome to submit and present at our workshop.</p>
        <!-- 单独的 Submission Guidelines -->
        <div style="margin-top: 2rem;">
            <h3 class="member-header" style="text-align: left;">Submission Guidelines</h3>
            <ul style="list-style-type: disc; padding-left: 20px; text-align: left;">
                <li>Submissions aligned with workshop themes</li>
                <li>Up to 7 pages (IJCAI 2025 main track format)</li>
                <li>Single round peer review process</li>
                <li>Presentation formats: Oral/Best Paper</li>
            </ul>
        </div>

        <!-- Key Topics 和 Related Research Topics 合并 -->
        <div style="margin-top: 2rem;">
            <h3 style="text-align: left;"><b>Key Topics</b></h3>
            <ul style="list-style-type: disc; padding-left: 20px; margin-bottom: 2rem; text-align: left;">
                <li>Multimodal Deepfake Detection</li>
                <li>LLM/VLM-based Methods</li>
                <li>Interpretability Analysis</li>
            </ul>

            <h3 style="text-align: left;"><b>Related Research Topics</b></h3>
            <ul style="list-style-type: none; padding-left: 0;text-align: left;">
                <li style="margin-bottom: 1rem;">
                    <strong>1. Detection Techniques for Risky AI-Generated Content:</strong>
                    <ul style="padding-left: 20px; list-style-type: none;">
                        <li>➡ Multi-Scene Deepfake Detection: Covering deepfake data across diverse scenarios, including face manipulation and generative content of natural objects.</li>
                        <li>➡ Multi-Modal Deepfake Detection: Addressing deepfake detection in text, image, audio, and video modalities.</li>
                        <li>➡ Safety Risk Content: Identifying generative content containing harmful elements, such as violent, terrorist, or explicit materials.</li>
                    </ul>
                </li>
                <li style="margin-bottom: 1rem;">
                    <strong>2. Interpretable Deepfake Detection Methods:</strong>
                    <ul style="padding-left: 20px; list-style-type: none;">
                        <li>➡ Weakly-/Unsupervised Deepfake Localization: Developing methods for detecting and localizing manipulations with minimal or no supervision.</li>
                        <li>➡ Interpretable Reasoning Frameworks via Large Models: Leveraging large language models (LLMs) and vision-language models (VLMs) to enhance interpretability in deepfake detection.</li>
                        <li>➡ Scalable Detection Analysis Agents: Designing agents capable of providing interpretable assessments for complex generative manipulations.</li>
                    </ul>
                </li>
                <li style="margin-bottom: 1rem;">
                    <strong>3. Adversarial Attack and Defense for Generative Large Models:</strong>
                    <ul style="padding-left: 20px; list-style-type: none;">
                        <li>➡ Attack Techniques: Exploring various attack vectors on generative large models, including prompt injections, jailbreak attacks, and fine-tuning manipulations.</li>
                        <li>➡ Risk-Trigger Mechanisms: Investigating the mechanisms that lead to the generation of risk-inducing content during generative model outputs.</li>
                        <li>➡ Robust Defense Strategies: Developing effective strategies to ensure the robustness of generative models in real-world applications.</li>
                    </ul>
                </li>
                <li style="margin-bottom: 1rem;">
                    <strong>4. Detection Methods for Advanced Face Spoofing Attacks:</strong>
                    <ul style="padding-left: 20px; list-style-type: none;">
                        <li>➡ Threat Analysis: Understanding how emerging facial spoofing techniques compromise biometric identification systems.</li>
                        <li>➡ Robust Detection for Physical Attacks: Constructing methods to reliably detect facial spoofing under challenging physical threats.</li>
                        <li>➡ Joint Detection Against Physical-Digital Attacks: Innovating detection methods for hybrid attacks that combine physical and digital manipulations.</li>
                    </ul>
                </li>
                <li style="margin-bottom: 1rem;">
                    <strong>5. Datasets and Evaluation Protocols:</strong>
                    <ul style="padding-left: 20px; list-style-type: none;">
                        <li>➡ Multi-Modal Spatiotemporal Datasets: Constructing datasets for locating image and video manipulations across time and space in multi-modal contexts.</li>
                        <li>➡ Safety Risk Assessment Datasets: Building datasets targeted at evaluating the risks posed by generative large models.</li>
                        <li>➡ Standardized Evaluation Protocols: Defining metrics and protocols to assess the interpretability and effectiveness of detection methods.</li>
                    </ul>
                </li>
            </ul>
        </div>
    </div>
    <!-- Awards Section -->
    <div class="highlight-card" style="padding: 2rem; margin-top: 2rem;">
        <h2 class="section-header" style="text-align: left;">
            <span class="icon">🏆</span>
            Awards
        </h2>
        <p style="text-align: left;"><strong>Award for Each Track:</strong></p>
        <ul style="list-style-type: none; padding-left: 0; text-align: left;">
            <li><i class="fas fa-trophy" style="color: gold;"></i> 1st-Place: Certificate + 1000 USD</li>
            <li><i class="fas fa-medal" style="color: silver;"></i> 2nd-Place: Certificate + 500 USD</li>
            <li><i class="fas fa-medal" style="color: #cd7f32;"></i> 3rd-Place: Certificate + 300 USD</li>
        </ul>
        <p style="text-align: left;"><strong>Award for Paper:</strong></p>
        <ul style="list-style-type: none; padding-left: 0; text-align: left;">
            <li><i class="fas fa-star" style="color: #FFD700;"></i> Best Paper: Certificate + 1000 USD</li>
        </ul>
    </div>
</main>

<footer>
    <p>Contact: deepfake.ijcai2025@gmail.com</p>
</footer>
</body>
</html>
